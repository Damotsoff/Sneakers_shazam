{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jufYMny2kbo",
        "outputId": "df8ac8fb-9e88-4d6e-9e1d-88e7f25a4bbc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDTvGt2zt7cm"
      },
      "source": [
        "# Configuring cuDNN on Colab for YOLOv4\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-bguKWgtxSx",
        "outputId": "126d19f0-0da0-43fc-fbd6-4f5865bb5c86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2020 NVIDIA Corporation\n",
            "Built on Mon_Oct_12_20:09:46_PDT_2020\n",
            "Cuda compilation tools, release 11.1, V11.1.105\n",
            "Build cuda_11.1.TC455_06.29190527_0\n"
          ]
        }
      ],
      "source": [
        "# CUDA: Let's check that Nvidia CUDA drivers are already pre-installed and which version is it.\n",
        "!/usr/local/cuda/bin/nvcc --version\n",
        "# We need to install the correct cuDNN according to this output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJYM7-_Had0Q",
        "outputId": "0dec9138-a187-4717-cb56-0bd796f57d55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fri Dec 10 08:57:33 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_qFPIBgvlkn",
        "outputId": "36965cc0-0a1c-48a9-e1b2-b94560c21fd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU Type: Tesla P100-PCIE-16GB\n",
            "\n",
            "ARCH Value: -gencode arch=compute_60,code=sm_60\n"
          ]
        }
      ],
      "source": [
        "# This cell ensures you have the correct architecture for your respective GPU\n",
        "# If you command is not found, look through these GPUs, find the respective\n",
        "# GPU and add them to the archTypes dictionary\n",
        "\n",
        "# Tesla V100\n",
        "# ARCH= -gencode arch=compute_70,code=[sm_70,compute_70]\n",
        "\n",
        "# Tesla K80 \n",
        "# ARCH= -gencode arch=compute_37,code=sm_37\n",
        "\n",
        "# GeForce RTX 2080 Ti, RTX 2080, RTX 2070, Quadro RTX 8000, Quadro RTX 6000, Quadro RTX 5000, Tesla T4, XNOR Tensor Cores\n",
        "# ARCH= -gencode arch=compute_75,code=[sm_75,compute_75]\n",
        "\n",
        "# Jetson XAVIER\n",
        "# ARCH= -gencode arch=compute_72,code=[sm_72,compute_72]\n",
        "\n",
        "# GTX 1080, GTX 1070, GTX 1060, GTX 1050, GTX 1030, Titan Xp, Tesla P40, Tesla P4\n",
        "# ARCH= -gencode arch=compute_61,code=sm_61\n",
        "\n",
        "# GP100/Tesla P100 - DGX-1\n",
        "# ARCH= -gencode arch=compute_60,code=sm_60\n",
        "\n",
        "# For Jetson TX1, Tegra X1, DRIVE CX, DRIVE PX - uncomment:\n",
        "# ARCH= -gencode arch=compute_53,code=[sm_53,compute_53]\n",
        "\n",
        "# For Jetson Tx2 or Drive-PX2 uncomment:\n",
        "# ARCH= -gencode arch=compute_62,code=[sm_62,compute_62]\n",
        "import os\n",
        "os.environ['GPU_TYPE'] = str(os.popen('nvidia-smi --query-gpu=name --format=csv,noheader').read())\n",
        "\n",
        "def getGPUArch(argument):\n",
        "  try:\n",
        "    argument = argument.strip()\n",
        "    # All Colab GPUs\n",
        "    archTypes = {\n",
        "        \"Tesla V100-SXM2-16GB\": \"-gencode arch=compute_70,code=[sm_70,compute_70]\",\n",
        "        \"Tesla K80\": \"-gencode arch=compute_37,code=sm_37\",\n",
        "        \"Tesla T4\": \"-gencode arch=compute_75,code=[sm_75,compute_75]\",\n",
        "        \"Tesla P40\": \"-gencode arch=compute_61,code=sm_61\",\n",
        "        \"Tesla P4\": \"-gencode arch=compute_61,code=sm_61\",\n",
        "        \"Tesla P100-PCIE-16GB\": \"-gencode arch=compute_60,code=sm_60\"\n",
        "\n",
        "      }\n",
        "    return archTypes[argument]\n",
        "  except KeyError:\n",
        "    return \"GPU must be added to GPU Commands\"\n",
        "os.environ['ARCH_VALUE'] = getGPUArch(os.environ['GPU_TYPE'])\n",
        "\n",
        "print(\"GPU Type: \" + os.environ['GPU_TYPE'])\n",
        "print(\"ARCH Value: \" + os.environ['ARCH_VALUE'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16pvdFMa1FEe"
      },
      "source": [
        "# Step 2: Installing Darknet for YOLOv4 on Colab\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9uY-38P93oz",
        "outputId": "28888b86-fc65-48d2-bf45-fa8884d93435"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "%cd /content\n",
        "%rm -rf darknet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQEktcfj9y9O",
        "outputId": "f05f6c78-1385-4779-94a8-8ee5331e731c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'darknet'...\n",
            "remote: Enumerating objects: 13289, done.\u001b[K\n",
            "remote: Total 13289 (delta 0), reused 0 (delta 0), pack-reused 13289\u001b[K\n",
            "Receiving objects: 100% (13289/13289), 12.16 MiB | 23.81 MiB/s, done.\n",
            "Resolving deltas: 100% (9047/9047), done.\n"
          ]
        }
      ],
      "source": [
        "#we clone the fork of darknet maintained by roboflow\n",
        "#small changes have been made to configure darknet for training\n",
        "!git clone https://github.com/roboflow-ai/darknet.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9O6dTiq5ga0I",
        "outputId": "2a003d89-06ab-44ac-c9fe-1141bd8d655d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/darknet\n"
          ]
        }
      ],
      "source": [
        "%cd /content/darknet/\n",
        "%rm Makefile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyTAyEhOgd__",
        "outputId": "dc1954e1-ce6f-433b-ef3c-f37d854e8310"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing Makefile\n"
          ]
        }
      ],
      "source": [
        "#colab occasionally shifts dependencies around, at the time of authorship, this Makefile works for building Darknet on Colab\n",
        "\n",
        "%%writefile Makefile\n",
        "GPU=1\n",
        "CUDNN=1\n",
        "CUDNN_HALF=0\n",
        "OPENCV=1\n",
        "AVX=0\n",
        "OPENMP=0\n",
        "LIBSO=1\n",
        "ZED_CAMERA=0\n",
        "ZED_CAMERA_v2_8=0\n",
        "\n",
        "# set GPU=1 and CUDNN=1 to speedup on GPU\n",
        "# set CUDNN_HALF=1 to further speedup 3 x times (Mixed-precision on Tensor Cores) GPU: Volta, Xavier, Turing and higher\n",
        "# set AVX=1 and OPENMP=1 to speedup on CPU (if error occurs then set AVX=0)\n",
        "# set ZED_CAMERA=1 to enable ZED SDK 3.0 and above\n",
        "# set ZED_CAMERA_v2_8=1 to enable ZED SDK 2.X\n",
        "\n",
        "USE_CPP=0\n",
        "DEBUG=0\n",
        "\n",
        "ARCH= -gencode arch=compute_35,code=sm_35 \\\n",
        "      -gencode arch=compute_50,code=[sm_50,compute_50] \\\n",
        "      -gencode arch=compute_52,code=[sm_52,compute_52] \\\n",
        "\t    -gencode arch=compute_61,code=[sm_61,compute_61] \\\n",
        "      -gencode arch=compute_37,code=sm_37\n",
        "\n",
        "ARCH= -gencode arch=compute_60,code=sm_60\n",
        "\n",
        "OS := $(shell uname)\n",
        "\n",
        "VPATH=./src/\n",
        "EXEC=darknet\n",
        "OBJDIR=./obj/\n",
        "\n",
        "ifeq ($(LIBSO), 1)\n",
        "LIBNAMESO=libdarknet.so\n",
        "APPNAMESO=uselib\n",
        "endif\n",
        "\n",
        "ifeq ($(USE_CPP), 1)\n",
        "CC=g++\n",
        "else\n",
        "CC=gcc\n",
        "endif\n",
        "\n",
        "CPP=g++ -std=c++11\n",
        "NVCC=nvcc\n",
        "OPTS=-Ofast\n",
        "LDFLAGS= -lm -pthread\n",
        "COMMON= -Iinclude/ -I3rdparty/stb/include\n",
        "CFLAGS=-Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC\n",
        "\n",
        "ifeq ($(DEBUG), 1)\n",
        "#OPTS= -O0 -g\n",
        "#OPTS= -Og -g\n",
        "COMMON+= -DDEBUG\n",
        "CFLAGS+= -DDEBUG\n",
        "else\n",
        "ifeq ($(AVX), 1)\n",
        "CFLAGS+= -ffp-contract=fast -mavx -mavx2 -msse3 -msse4.1 -msse4.2 -msse4a\n",
        "endif\n",
        "endif\n",
        "\n",
        "CFLAGS+=$(OPTS)\n",
        "\n",
        "ifneq (,$(findstring MSYS_NT,$(OS)))\n",
        "LDFLAGS+=-lws2_32\n",
        "endif\n",
        "\n",
        "ifeq ($(OPENCV), 1)\n",
        "COMMON+= -DOPENCV\n",
        "CFLAGS+= -DOPENCV\n",
        "LDFLAGS+= `pkg-config --libs opencv4 2> /dev/null || pkg-config --libs opencv`\n",
        "COMMON+= `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv`\n",
        "endif\n",
        "\n",
        "ifeq ($(OPENMP), 1)\n",
        "CFLAGS+= -fopenmp\n",
        "LDFLAGS+= -lgomp\n",
        "endif\n",
        "\n",
        "ifeq ($(GPU), 1)\n",
        "COMMON+= -DGPU -I/usr/local/cuda/include/\n",
        "CFLAGS+= -DGPU\n",
        "ifeq ($(OS),Darwin) #MAC\n",
        "LDFLAGS+= -L/usr/local/cuda/lib -lcuda -lcudart -lcublas -lcurand\n",
        "else\n",
        "LDFLAGS+= -L/usr/local/cuda/lib64 -lcuda -lcudart -lcublas -lcurand\n",
        "endif\n",
        "endif\n",
        "\n",
        "ifeq ($(CUDNN), 1)\n",
        "COMMON+= -DCUDNN\n",
        "ifeq ($(OS),Darwin) #MAC\n",
        "CFLAGS+= -DCUDNN -I/usr/local/cuda/include\n",
        "LDFLAGS+= -L/usr/local/cuda/lib -lcudnn\n",
        "else\n",
        "CFLAGS+= -DCUDNN -I/usr/local/cudnn/include\n",
        "LDFLAGS+= -L/usr/local/cudnn/lib64 -lcudnn\n",
        "endif\n",
        "endif\n",
        "\n",
        "ifeq ($(CUDNN_HALF), 1)\n",
        "COMMON+= -DCUDNN_HALF\n",
        "CFLAGS+= -DCUDNN_HALF\n",
        "ARCH+= -gencode arch=compute_70,code=[sm_70,compute_70]\n",
        "endif\n",
        "\n",
        "ifeq ($(ZED_CAMERA), 1)\n",
        "CFLAGS+= -DZED_STEREO -I/usr/local/zed/include\n",
        "ifeq ($(ZED_CAMERA_v2_8), 1)\n",
        "LDFLAGS+= -L/usr/local/zed/lib -lsl_core -lsl_input -lsl_zed\n",
        "#-lstdc++ -D_GLIBCXX_USE_CXX11_ABI=0\n",
        "else\n",
        "LDFLAGS+= -L/usr/local/zed/lib -lsl_zed\n",
        "#-lstdc++ -D_GLIBCXX_USE_CXX11_ABI=0\n",
        "endif\n",
        "endif\n",
        "\n",
        "OBJ=image_opencv.o http_stream.o gemm.o utils.o dark_cuda.o convolutional_layer.o list.o image.o activations.o im2col.o col2im.o blas.o crop_layer.o dropout_layer.o maxpool_layer.o softmax_layer.o data.o matrix.o network.o connected_layer.o cost_layer.o parser.o option_list.o darknet.o detection_layer.o captcha.o route_layer.o writing.o box.o nightmare.o normalization_layer.o avgpool_layer.o coco.o dice.o yolo.o detector.o layer.o compare.o classifier.o local_layer.o swag.o shortcut_layer.o activation_layer.o rnn_layer.o gru_layer.o rnn.o rnn_vid.o crnn_layer.o demo.o tag.o cifar.o go.o batchnorm_layer.o art.o region_layer.o reorg_layer.o reorg_old_layer.o super.o voxel.o tree.o yolo_layer.o gaussian_yolo_layer.o upsample_layer.o lstm_layer.o conv_lstm_layer.o scale_channels_layer.o sam_layer.o\n",
        "ifeq ($(GPU), 1)\n",
        "LDFLAGS+= -lstdc++\n",
        "OBJ+=convolutional_kernels.o activation_kernels.o im2col_kernels.o col2im_kernels.o blas_kernels.o crop_layer_kernels.o dropout_layer_kernels.o maxpool_layer_kernels.o network_kernels.o avgpool_layer_kernels.o\n",
        "endif\n",
        "\n",
        "OBJS = $(addprefix $(OBJDIR), $(OBJ))\n",
        "DEPS = $(wildcard src/*.h) Makefile include/darknet.h\n",
        "\n",
        "all: $(OBJDIR) backup results setchmod $(EXEC) $(LIBNAMESO) $(APPNAMESO)\n",
        "\n",
        "ifeq ($(LIBSO), 1)\n",
        "CFLAGS+= -fPIC\n",
        "\n",
        "$(LIBNAMESO): $(OBJDIR) $(OBJS) include/yolo_v2_class.hpp src/yolo_v2_class.cpp\n",
        "\t$(CPP) -shared -std=c++11 -fvisibility=hidden -DLIB_EXPORTS $(COMMON) $(CFLAGS) $(OBJS) src/yolo_v2_class.cpp -o $@ $(LDFLAGS)\n",
        "\n",
        "$(APPNAMESO): $(LIBNAMESO) include/yolo_v2_class.hpp src/yolo_console_dll.cpp\n",
        "\t$(CPP) -std=c++11 $(COMMON) $(CFLAGS) -o $@ src/yolo_console_dll.cpp $(LDFLAGS) -L ./ -l:$(LIBNAMESO)\n",
        "endif\n",
        "\n",
        "$(EXEC): $(OBJS)\n",
        "\t$(CPP) -std=c++11 $(COMMON) $(CFLAGS) $^ -o $@ $(LDFLAGS)\n",
        "\n",
        "$(OBJDIR)%.o: %.c $(DEPS)\n",
        "\t$(CC) $(COMMON) $(CFLAGS) -c $< -o $@\n",
        "\n",
        "$(OBJDIR)%.o: %.cpp $(DEPS)\n",
        "\t$(CPP) -std=c++11 $(COMMON) $(CFLAGS) -c $< -o $@\n",
        "\n",
        "$(OBJDIR)%.o: %.cu $(DEPS)\n",
        "\t$(NVCC) $(ARCH) $(COMMON) --compiler-options \"$(CFLAGS)\" -c $< -o $@\n",
        "\n",
        "$(OBJDIR):\n",
        "\tmkdir -p $(OBJDIR)\n",
        "backup:\n",
        "\tmkdir -p backup\n",
        "results:\n",
        "\tmkdir -p results\n",
        "setchmod:\n",
        "\tchmod +x *.sh\n",
        "\n",
        ".PHONY: clean\n",
        "\n",
        "clean:\n",
        "\trm -rf $(OBJS) $(EXEC) $(LIBNAMESO) $(APPNAMESO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyMBDkaL-Aep"
      },
      "outputs": [],
      "source": [
        "#install environment from the Makefile\n",
        "#note if you are on Colab Pro this works on a P100 GPU\n",
        "#if you are on Colab free, you may need to change the Makefile for the K80 GPU\n",
        "#this goes for any GPU, you need to change the Makefile to inform darknet which GPU you are running on.\n",
        "#note the Makefile above should work for you, if you need to tweak, try the below\n",
        "%cd /content/darknet\n",
        "!sed -i 's/OPENCV=0/OPENCV=1/g' Makefile\n",
        "!sed -i 's/GPU=0/GPU=1/g' Makefile\n",
        "!sed -i 's/CUDNN=0/CUDNN=1/g' Makefile\n",
        "!sed -i \"s/ARCH= -gencode arch=compute_60,code=sm_60/ARCH= ${ARCH_VALUE}/g\" Makefile\n",
        "!make"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGPDEjfAALrQ",
        "outputId": "8ec76a1f-b6bb-4aed-c4da-ca834f1589d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/darknet\n",
            "--2021-12-10 08:59:24--  https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.conv.137\n",
            "Resolving github.com (github.com)... 192.30.255.113\n",
            "Connecting to github.com (github.com)|192.30.255.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/75388965/48bfe500-889d-11ea-819e-c4d182fcf0db?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20211210%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20211210T085924Z&X-Amz-Expires=300&X-Amz-Signature=a8612154db1579550facdb35bf66bdac3b4fe044146efd73132df3919722f1db&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=75388965&response-content-disposition=attachment%3B%20filename%3Dyolov4.conv.137&response-content-type=application%2Foctet-stream [following]\n",
            "--2021-12-10 08:59:24--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/75388965/48bfe500-889d-11ea-819e-c4d182fcf0db?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20211210%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20211210T085924Z&X-Amz-Expires=300&X-Amz-Signature=a8612154db1579550facdb35bf66bdac3b4fe044146efd73132df3919722f1db&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=75388965&response-content-disposition=attachment%3B%20filename%3Dyolov4.conv.137&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 170038676 (162M) [application/octet-stream]\n",
            "Saving to: ‘yolov4.conv.137’\n",
            "\n",
            "yolov4.conv.137     100%[===================>] 162.16M  57.2MB/s    in 2.8s    \n",
            "\n",
            "2021-12-10 08:59:27 (57.2 MB/s) - ‘yolov4.conv.137’ saved [170038676/170038676]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#download the newly released yolov4 ConvNet weights\n",
        "%cd /content/darknet\n",
        "!wget https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.conv.137"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWOiKj37l4wW"
      },
      "source": [
        "# Set up Custom Dataset for YOLOv4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KiQvussOcXdJ"
      },
      "outputs": [],
      "source": [
        "#follow the link below to get your download code from from Roboflow\n",
        "!pip install -q roboflow\n",
        "# from roboflow import Roboflow\n",
        "# rf = Roboflow(model_format=\"darknet\", notebook=\"roboflow-yolov4\")\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"hRTCFzw2Ax720OnbMBHC\")\n",
        "project = rf.workspace().project(\"sneakers-k7svc\")\n",
        "dataset = project.version(2).download(\"darknet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "1ok3Snnk6cnm",
        "outputId": "1d2001be-b57e-4245-d135-616659683870"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/darknet/Sneakers-2'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset.location"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ueSd5oLs8Dan"
      },
      "outputs": [],
      "source": [
        "dataset.location = '/content/drive/MyDrive/darknet/Sneakers-2'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KiCILEbs1NII",
        "outputId": "0ee4565b-21a3-4712-c3cb-eb501d5da380"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/darknet\n"
          ]
        }
      ],
      "source": [
        "#Set up training file directories for custom dataset\n",
        "%cd /content/darknet/\n",
        "%cp {dataset.location}/train/_darknet.labels data/obj.names\n",
        "%mkdir data/obj\n",
        "#copy image and labels\n",
        "%cp {dataset.location}/train/*.jpg data/obj/\n",
        "%cp {dataset.location}/valid/*.jpg data/obj/\n",
        "\n",
        "%cp {dataset.location}/train/*.txt data/obj/\n",
        "%cp {dataset.location}/valid/*.txt data/obj/\n",
        "\n",
        "with open('data/obj.data', 'w') as out:\n",
        "  out.write('classes = 1\\n')\n",
        "  out.write('train = data/train.txt\\n')\n",
        "  out.write('valid = data/valid.txt\\n')\n",
        "  out.write('names = data/obj.names\\n')\n",
        "  out.write('backup = backup/')\n",
        "\n",
        "#write train file (just the image list)\n",
        "import os\n",
        "\n",
        "with open('data/train.txt', 'w') as out:\n",
        "  for img in [f for f in os.listdir(dataset.location + '/train') if f.endswith('jpg')]:\n",
        "    out.write('data/obj/' + img + '\\n')\n",
        "\n",
        "#write the valid file (just the image list)\n",
        "import os\n",
        "\n",
        "with open('data/valid.txt', 'w') as out:\n",
        "  for img in [f for f in os.listdir(dataset.location + '/valid') if f.endswith('jpg')]:\n",
        "    out.write('data/obj/' + img + '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HtRqO3QvjkP"
      },
      "source": [
        "# Write Custom Training Config for YOLOv4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_WJcqHhpeVr",
        "outputId": "7d2dfc48-e9ea-43f7-a3ca-c4fa0cd2632e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "writing config for a custom YOLOv4 detector detecting number of classes: 1\n",
            "file is written!\n"
          ]
        }
      ],
      "source": [
        "#we build config dynamically based on number of classes\n",
        "#we build iteratively from base config files. This is the same file shape as cfg/yolo-obj.cfg\n",
        "def file_len(fname):\n",
        "  with open(fname) as f:\n",
        "    for i, l in enumerate(f):\n",
        "      pass\n",
        "  return i + 1\n",
        "\n",
        "num_classes = file_len(dataset.location + '/train/_darknet.labels')\n",
        "print(\"writing config for a custom YOLOv4 detector detecting number of classes: \" + str(num_classes))\n",
        "\n",
        "#Instructions from the darknet repo\n",
        "#change line max_batches to (classes*2000 but not less than number of training images, and not less than 6000), f.e. max_batches=6000 if you train for 3 classes\n",
        "#change line steps to 80% and 90% of max_batches, f.e. steps=4800,5400\n",
        "if os.path.exists('./cfg/custom-yolov4-detector.cfg'): os.remove('./cfg/custom-yolov4-detector.cfg')\n",
        "\n",
        "\n",
        "with open('./cfg/custom-yolov4-detector.cfg', 'a') as f:\n",
        "  f.write('[net]' + '\\n')\n",
        "  f.write('batch=64' + '\\n')\n",
        "  #####smaller subdivisions help the GPU run faster. 12 is optimal, but you might need to change to 24,36,64####\n",
        "  f.write('subdivisions=24' + '\\n')\n",
        "  f.write('width=416' + '\\n')\n",
        "  f.write('height=416' + '\\n')\n",
        "  f.write('channels=3' + '\\n')\n",
        "  f.write('momentum=0.949' + '\\n')\n",
        "  f.write('decay=0.0005' + '\\n')\n",
        "  f.write('angle=0' + '\\n')\n",
        "  f.write('saturation = 0' + '\\n')\n",
        "  f.write('exposure = 0' + '\\n')\n",
        "  f.write('hue = 0' + '\\n')\n",
        "  f.write('\\n')\n",
        "  f.write('learning_rate=0.0005' + '\\n')\n",
        "  f.write('burn_in=1000' + '\\n')\n",
        "  ######you can adjust up and down to change training time#####\n",
        "  ##Darknet does iterations with batches, not epochs####\n",
        "  # max_batches = num_classes*2000\n",
        "  max_batches = 5500\n",
        "  f.write('max_batches=' + str(max_batches) + '\\n')\n",
        "  f.write('policy=steps' + '\\n')\n",
        "  steps1 = .8 * max_batches\n",
        "  steps2 = .9 * max_batches\n",
        "  f.write('steps='+str(steps1)+','+str(steps2) + '\\n')\n",
        "\n",
        "#Instructions from the darknet repo\n",
        "#change line classes=80 to your number of objects in each of 3 [yolo]-layers:\n",
        "#change [filters=255] to filters=(classes + 5)x3 in the 3 [convolutional] before each [yolo] layer, keep in mind that it only has to be the last [convolutional] before each of the [yolo] layers.\n",
        "\n",
        "  with open('cfg/yolov4-custom2.cfg', 'r') as f2:\n",
        "    content = f2.readlines()\n",
        "    for line in content:\n",
        "      f.write(line)    \n",
        "    num_filters = (num_classes + 5) * 3\n",
        "    f.write('filters='+str(num_filters) + '\\n')\n",
        "    f.write('activation=linear')\n",
        "    f.write('\\n')\n",
        "    f.write('\\n')\n",
        "    f.write('[yolo]' + '\\n')\n",
        "    f.write('mask = 0,1,2' + '\\n')\n",
        "    f.write('anchors = 12, 16, 19, 36, 40, 28, 36, 75, 76, 55, 72, 146, 142, 110, 192, 243, 459, 401' + '\\n')\n",
        "    f.write('classes=' + str(num_classes) + '\\n')\n",
        "\n",
        "  with open('cfg/yolov4-custom3.cfg', 'r') as f3:\n",
        "    content = f3.readlines()\n",
        "    for line in content:\n",
        "      f.write(line)    \n",
        "    num_filters = (num_classes + 5) * 3\n",
        "    f.write('filters='+str(num_filters) + '\\n')\n",
        "    f.write('activation=linear')\n",
        "    f.write('\\n')\n",
        "    f.write('\\n')\n",
        "    f.write('[yolo]' + '\\n')\n",
        "    f.write('mask = 3,4,5' + '\\n')\n",
        "    f.write('anchors = 12, 16, 19, 36, 40, 28, 36, 75, 76, 55, 72, 146, 142, 110, 192, 243, 459, 401' + '\\n')\n",
        "    f.write('classes=' + str(num_classes) + '\\n')\n",
        "\n",
        "  with open('cfg/yolov4-custom4.cfg', 'r') as f4:\n",
        "    content = f4.readlines()\n",
        "    for line in content:\n",
        "      f.write(line)    \n",
        "    num_filters = (num_classes + 5) * 3\n",
        "    f.write('filters='+str(num_filters) + '\\n')\n",
        "    f.write('activation=linear')\n",
        "    f.write('\\n')\n",
        "    f.write('\\n')\n",
        "    f.write('[yolo]' + '\\n')\n",
        "    f.write('mask = 6,7,8' + '\\n')\n",
        "    f.write('anchors = 12, 16, 19, 36, 40, 28, 36, 75, 76, 55, 72, 146, 142, 110, 192, 243, 459, 401' + '\\n')\n",
        "    f.write('classes=' + str(num_classes) + '\\n')\n",
        "    \n",
        "  with open('cfg/yolov4-custom5.cfg', 'r') as f5:\n",
        "    content = f5.readlines()\n",
        "    for line in content:\n",
        "      f.write(line)\n",
        "\n",
        "print(\"file is written!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2LAciMh4Cut"
      },
      "outputs": [],
      "source": [
        "#here is the file that was just written. \n",
        "#you may consider adjusting certain things\n",
        "\n",
        "#like the number of subdivisions 64 runs faster but Colab GPU may not be big enough\n",
        "#if Colab GPU memory is too small, you will need to adjust subdivisions to 16\n",
        "%cat cfg/custom-yolov4-detector.cfg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWrG9EGamSpH"
      },
      "source": [
        "# Train Custom YOLOv4 Detector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6miYFbvExqMd"
      },
      "outputs": [],
      "source": [
        "!./darknet detector train data/obj.data cfg/custom-yolov4-detector.cfg backup/custom-yolov4-detector_final.weights -dont_show -map\n",
        "#If you get CUDA out of memory adjust subdivisions above!\n",
        "#adjust max batches down for shorter training above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBnwpBV5ZXxQ"
      },
      "source": [
        "# Infer Custom Objects with Saved YOLOv4 Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FzoJQQw8Zdco"
      },
      "outputs": [],
      "source": [
        "#define utility function\n",
        "def imShow(path):\n",
        "  import cv2\n",
        "  import matplotlib.pyplot as plt\n",
        "  %matplotlib inline\n",
        "\n",
        "  image = cv2.imread(path)\n",
        "  height, width = image.shape[:2]\n",
        "  resized_image = cv2.resize(image,(3*width, 3*height), interpolation = cv2.INTER_CUBIC)\n",
        "\n",
        "  fig = plt.gcf()\n",
        "  fig.set_size_inches(18, 10)\n",
        "  plt.axis(\"off\")\n",
        "  #plt.rcParams['figure.figsize'] = [10, 5]\n",
        "  plt.imshow(cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB))\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3dJB6NZv4kh"
      },
      "outputs": [],
      "source": [
        "#check if weigths have saved yet\n",
        "#backup houses the last weights for our detector\n",
        "#(file yolo-obj_last.weights will be saved to the build\\darknet\\x64\\backup\\ for each 100 iterations)\n",
        "#(file yolo-obj_xxxx.weights will be saved to the build\\darknet\\x64\\backup\\ for each 1000 iterations)\n",
        "#After training is complete - get result yolo-obj_final.weights from path build\\darknet\\x64\\bac\n",
        "!ls backup\n",
        "#if it is empty you haven't trained for long enough yet, you need to train for at least 100 iterations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-_E3O5Mf4Mf"
      },
      "outputs": [],
      "source": [
        "#coco.names is hardcoded somewhere in the detector\n",
        "%cp data/obj.names data/coco.names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "617Hf4HRhbzy"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgFcWVdpHix2"
      },
      "outputs": [],
      "source": [
        "# задаем пути к папкам с моделями\n",
        "\n",
        "path_to_two_models ='/content/drive/MyDrive/ebay_sneakers/OG_photos/Max_n_Ultra'\n",
        "two_path = [x[0] for x in os.walk(path_to_two_models)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTIFlDdkay4_",
        "outputId": "8538d12b-578f-4cc4-910c-b38d227cdd08"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/ebay_sneakers/OG_photos/Max_n_Ultra',\n",
              " '/content/drive/MyDrive/ebay_sneakers/OG_photos/Max_n_Ultra/Adidas_Ultra_Boost_4.0',\n",
              " '/content/drive/MyDrive/ebay_sneakers/OG_photos/Max_n_Ultra/Nike_Air_Max_1']"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "two_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9wl0EYkHZEN"
      },
      "outputs": [],
      "source": [
        "# Абсолютный путь для каждой фотки в папках\n",
        "\n",
        "two_abs_paths = [os.path.join(path, i) for path in two_path[1:] for i in os.listdir(path)]\n",
        "\n",
        "with open('/content/drive/MyDrive/ebay_sneakers/OG_photos/project_two_path_to_images.txt', 'w') as w_f:\n",
        "    for path in two_abs_paths:\n",
        "        w_f.write(f'{path}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1ikZhIgwCui"
      },
      "outputs": [],
      "source": [
        "#Model Ebay photos testing\n",
        "\n",
        "\n",
        "images = \"/content/drive/MyDrive/ebay_sneakers/OG_photos/project_two_path_to_images.txt\"\n",
        "\n",
        "#test out our detector!\n",
        "!./darknet detector test data/obj.data /content/drive/MyDrive/sneaker_weights/custom-yolov4-detector.cfg /content/drive/MyDrive/sneaker_weights/custom-yolov4-detector_best-2.weights -thresh 0.6 -dont_show -ext_output < {images} > /content/drive/MyDrive/ebay_sneakers/OG_photos/project_two_result.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jp0ERKDV3SZW"
      },
      "source": [
        "**For Test**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iW2n5HEh3aa_"
      },
      "outputs": [],
      "source": [
        "# path_to_all_models = '/content/drive/MyDrive/ebay_sneakers/OG_photos'\n",
        "path_to_example_models = '/content/drive/MyDrive/ebay_sneakers/OG_photos/exmaple'\n",
        "# models_list =  os.listdir(path_to_all_models)\n",
        "# all_pathes = [os.listdir(os.path.abspath(i)) for i in models_list]\n",
        "example_path = [x[0] for x in os.walk(path_to_example_models)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2lea-7Q3abA"
      },
      "outputs": [],
      "source": [
        "example_class = [os.path.join(example_path[0], i) for i in os.listdir(example_path[0])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kV2RCa4b4GVB"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/ebay_sneakers/path_to_example_images.txt', 'w') as w_f:\n",
        "    for path in unknown_class:\n",
        "        w_f.write(f'{path}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pYAphE73abA",
        "outputId": "29e8da87-33e2-41b8-b05c-34494240b04c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: ./darknet: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "# predict for tests\n",
        "# img_path = \"/content/drive/MyDrive/тесты/dima.jpeg\"\n",
        "images = \"/content/drive/MyDrive/ebay_sneakers/example/ex_res.txt\"\n",
        "\n",
        "#test out our detector!\n",
        "!./darknet detector test data/obj.data /content/drive/MyDrive/sneaker_weights/custom-yolov4-detector.cfg /content/drive/MyDrive/sneaker_weights/custom-yolov4-detector_best-2.weights -thresh 0.6 -dont_show -ext_output < {images} > /content/drive/MyDrive/ebay_sneakers/example/example_result.txt\n",
        "# imShow('/content/darknet/predictions.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vZQtYu1dA-s",
        "outputId": "115735d7-8202-4720-9fc1-65c7d131f3b5"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import itertools\n",
        "import random\n",
        "import os\n",
        "\n",
        "\n",
        "path_cropped_images = '/content/drive/MyDrive/ebay_sneakers/project/'\n",
        "res = []\n",
        "result_dict = {}\n",
        "line_id_img = []\n",
        "\n",
        "\n",
        "def crop_image(input_image, coords, cropped_image):\n",
        "    x, y, w, h = coords\n",
        "    croped_shoe = Image.open(input_image)\n",
        "    croped_shoe = croped_shoe.crop((x, y, x + w, y + h)).resize((224,224)).save(cropped_image)\n",
        "\n",
        "\n",
        "with open('/content/drive/MyDrive/ebay_sneakers/OG_photos/project_two_result.txt') as f:\n",
        "    lines = f.read().split('\\n')\n",
        "    lines = list(filter(None, lines))\n",
        "    \n",
        "    for line, text in enumerate(lines):\n",
        "        if text.startswith('Enter'):\n",
        "            line_id_img.append(line + 1)\n",
        "        \n",
        "    line_id_img_dif = list(itertools.starmap(lambda x, y: y - x - 1, zip(line_id_img, line_id_img[1:])))\n",
        "    line_id_img = line_id_img[:-1]\n",
        "\n",
        "    for i, j in zip(line_id_img, line_id_img_dif):\n",
        "        res.append(lines[i:i + j])\n",
        "\n",
        "    for k in range(len(line_id_img)):\n",
        "        images = lines[line_id_img[k] - 1].split(':')[1].strip()\n",
        "        coords = [i.split('(')[1].strip().replace(')', '') for i in res[k]]\n",
        "        out = [l.split() for l in coords]\n",
        "        int_coords = [tuple(int(x) for x in out[i] if x.isnumeric()) for i in range(len(out))]\n",
        "        result_dict.update({images: int_coords})\n",
        "\n",
        "print(result_dict)\n",
        "\n",
        "\n",
        "for k, v in result_dict.items():\n",
        "    sneaker_name = k.split('/')[-2]\n",
        "    sneaker_path = os.path.join(path_cropped_images, sneaker_name)\n",
        "    if not os.path.exists(sneaker_path):\n",
        "        os.mkdir(sneaker_path)\n",
        "    for i in v:\n",
        "        try:\n",
        "          crop_image(k, i, sneaker_path + '/' + str(random.randint(1111, 9999)) + k.split('/')[-1])\n",
        "        except Exception as e:\n",
        "            continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VbrLNEdTSHPX"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, BatchNormalization\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUOVLeZFSHPX"
      },
      "outputs": [],
      "source": [
        "path_to_train = '/content/drive/MyDrive/ebay_sneakers/project/Train'\n",
        "path_to_valid = '/content/drive/MyDrive/ebay_sneakers/project/Valid'\n",
        "\n",
        "# path_to_test = '/content/drive/MyDrive/ebay_sneakers/cropped_images/for_test' # для предсказаний тренировочных\n",
        "# new_path_to_learning = '/content/drive/MyDrive/ebay_sneakers/cropped_images/new_learning' # путь до лернинга"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iya7jUHTitQ8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "path_to_models = \"/content/drive/MyDrive/ebay_sneakers/project\"           #пути к кропнутым фоткам\n",
        "# path_to_new_models = \"/content/drive/MyDrive/ebay_sneakers/cropped_images/for_test\"\n",
        "models_list =  os.listdir(path_to_models)                                # пути ко всем папкам\n",
        "name_jpg = [i for i in [x for x in os.walk(path_to_models)]]\n",
        "abs_path = [os.path.join(name[0], i) for name in name_jpg for i in name[2]]\n",
        "# abs_path = [os.path.join(path_to_models, i) for i in name_jpg]         #абсолютный путь для каждой кропнутой фотки"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yq3rGcWOSHPY",
        "outputId": "24920574-d73e-454a-f2d0-0ec662331a84"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/ebay_sneakers/project/Nike_Air_Force_1_Low',\n",
              " '/content/drive/MyDrive/ebay_sneakers/project/Nike_Air_Max_90',\n",
              " '/content/drive/MyDrive/ebay_sneakers/project/Nike_Air_Huarache',\n",
              " '/content/drive/MyDrive/ebay_sneakers/project/Reebok_Instapump_Fury',\n",
              " '/content/drive/MyDrive/ebay_sneakers/project/Nike_Air_Max_95',\n",
              " '/content/drive/MyDrive/ebay_sneakers/project/Nike_Blazer',\n",
              " '/content/drive/MyDrive/ebay_sneakers/project/Air_Jordan_11',\n",
              " '/content/drive/MyDrive/ebay_sneakers/project/Air_Jordan_4',\n",
              " '/content/drive/MyDrive/ebay_sneakers/project/Adidas_NMD_R1',\n",
              " '/content/drive/MyDrive/ebay_sneakers/project/Unknown',\n",
              " '/content/drive/MyDrive/ebay_sneakers/project/Adidas_Yeezy_Boost_350',\n",
              " '/content/drive/MyDrive/ebay_sneakers/project/Adidas_Yeezy_Boost_700',\n",
              " '/content/drive/MyDrive/ebay_sneakers/project/Nike_Dunk_low',\n",
              " '/content/drive/MyDrive/ebay_sneakers/project/Air_Jordan_12',\n",
              " '/content/drive/MyDrive/ebay_sneakers/project/Air_Jordan_6',\n",
              " '/content/drive/MyDrive/ebay_sneakers/project/Air_Jordan_13',\n",
              " '/content/drive/MyDrive/ebay_sneakers/project/Air_Jordan_1_Retro_High',\n",
              " '/content/drive/MyDrive/ebay_sneakers/project/Nike_Air_VaporMax',\n",
              " '/content/drive/MyDrive/ebay_sneakers/project/Adidas_Ultra_Boost_4.0',\n",
              " '/content/drive/MyDrive/ebay_sneakers/project/Nike_Air_Max_1']"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "abs_folders_path_to_models = [os.path.join(path_to_models, i) for i in os.listdir(path_to_models) if i != 'Valid' and i != 'Train' \n",
        "                              and i != 'checkpoint' and i != 'extra']\n",
        "abs_folders_path_to_models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Classification by ResNet101V2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2GDBPCmIj0v"
      },
      "source": [
        "#### Split data on valid(0.2) and train(0.8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVcQNn1ESHPY"
      },
      "outputs": [],
      "source": [
        "for folder in abs_folders_path_to_models:\n",
        "  k = len(os.listdir(folder)) * 20 // 100 \n",
        "  indicies = random.sample(range(len(os.listdir(folder))), k)\n",
        "  valid_photos = [os.path.join(folder, os.listdir(folder)[i]) for i in indicies]\n",
        "  for vl in valid_photos: \n",
        "    path_to_valid_model = os.path.join(path_to_valid, vl.split('/')[-2])\n",
        "    if not os.path.exists(path_to_valid_model):\n",
        "      os.mkdir(path_to_valid_model) \n",
        "    os.rename(vl, os.path.join(path_to_valid_model, vl.split('/')[-1]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyijU7Ncoyqz"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
        "from tensorflow.keras.applications import InceptionResNetV2\n",
        "from tensorflow.keras.applications import NASNetLarge, ResNet101V2\n",
        "# from tensorflow.keras.applications.na\n",
        "from tensorflow.keras.preprocessing import image, image_dataset_from_directory\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
        "import numpy as np\n",
        "from operator import itemgetter "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Data generator and augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "236dfbc9-4a0b-4736-aac4-d4ded9aa0de0",
        "outputId": "72cf3818-36c4-44e6-cd32-15061a46d521"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 2969 images belonging to 20 classes.\n",
            "Found 729 images belonging to 20 classes.\n"
          ]
        }
      ],
      "source": [
        "train_data_gen = image.ImageDataGenerator(rescale=1/255., \n",
        "                                        shear_range=0.2,\n",
        "                                        zoom_range=0.2,\n",
        "                                        horizontal_flip=True,\n",
        "                                        vertical_flip=True,\n",
        "                                        rotation_range=90,\n",
        "                                        validation_split=0.2).flow_from_directory('/content/drive/MyDrive/ebay_sneakers/project/Train')\n",
        "val_data_gen = image.ImageDataGenerator(rescale=1/255.).flow_from_directory('/content/drive/MyDrive/ebay_sneakers/project/Valid')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Model init and compile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3e1415fd-c522-40ac-b820-d84ab6b288a5"
      },
      "outputs": [],
      "source": [
        "inception = ResNet101V2(weights='imagenet', include_top=False)\n",
        "x = inception.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "x = Dropout(.3)(x)\n",
        "preds = Dense(20, activation='softmax')(x)\n",
        "model = Model(inputs=inception.input, outputs=preds)\n",
        "\n",
        "for layer in inception.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "\n",
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), \n",
        "              loss='categorical_crossentropy', \n",
        "              metrics=['accuracy',keras.metrics.TopKCategoricalAccuracy(k=4, name=\"top_4_accuracy\")])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ks5zz24Hue3P"
      },
      "outputs": [],
      "source": [
        "checkpoint = keras.callbacks.ModelCheckpoint(filepath = '/content/drive/MyDrive/ebay_sneakers/project/checkpoint/', monitor='val_top_4_accuracy', verbose=1, save_best_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdQiT-IA5MLC"
      },
      "outputs": [],
      "source": [
        "my_path = '/content/drive/MyDrive/ebay_sneakers/project/Train/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Check for broken files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uc7Hogaw4A1w"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "num_skipped = 0\n",
        "for folder_name in (os.listdir(my_path)):\n",
        "    folder_path = os.path.join(my_path, folder_name)\n",
        "    print(folder_name)\n",
        "    for fname in os.listdir(folder_path):\n",
        "        fpath = os.path.join(folder_path, fname)\n",
        "        try:\n",
        "            fobj = open(fpath, \"rb\")\n",
        "            is_jfif = tf.compat.as_bytes(\"JFIF\") in fobj.peek(10)\n",
        "        finally:\n",
        "            fobj.close()\n",
        "\n",
        "        if not is_jfif:\n",
        "            num_skipped += 1\n",
        "            # Delete corrupted image\n",
        "            os.remove(fpath)\n",
        "\n",
        "print(\"Deleted %d images\" % num_skipped)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eg_IM8T0ue3Q"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "      train_data_gen,\n",
        "      callbacks = checkpoint,\n",
        "      epochs=10,\n",
        "      validation_data=val_data_gen,\n",
        "      )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKIqHQPRNO_U"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import image, image_dataset_from_directory\n",
        "from operator import itemgetter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzoUchXcONsI"
      },
      "outputs": [],
      "source": [
        "# from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
        "import numpy as np\n",
        "\n",
        "img_path = '/content/drive/MyDrive/ebay_sneakers/cropped_images/for_test/new_test/nt_2/6334adidas_sl80_3.jpg'\n",
        "img = image.load_img(img_path, target_size=(224, 224))\n",
        "# img = img.rescale(1/255.)\n",
        "x = image.img_to_array(img).astype('float32') / 255\n",
        "x = np.expand_dims(x, axis=0)\n",
        "\n",
        "preds = model.predict(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NN0IkJuqONsM"
      },
      "outputs": [],
      "source": [
        "def get_labels(preds, n=4): \n",
        "    preds = list(reversed(np.argsort(preds, axis=1)[:,-4:][0]))\n",
        "    print(preds)\n",
        "    labels = list(train_data_gen.class_indices.keys())\n",
        "    selector = itemgetter(*preds)\n",
        "    return selector(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuXQ65AZAFhc",
        "outputId": "c265cdd0-a77c-4d5a-e0d4-8a220d6a8523"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Adidas_NMD_R1': 0,\n",
              " 'Adidas_Ultra_Boost_4.0': 1,\n",
              " 'Adidas_Yeezy_Boost_350': 2,\n",
              " 'Adidas_Yeezy_Boost_700': 3,\n",
              " 'Air_Jordan_11': 4,\n",
              " 'Air_Jordan_12': 5,\n",
              " 'Air_Jordan_13': 6,\n",
              " 'Air_Jordan_1_Retro_High': 7,\n",
              " 'Air_Jordan_4': 8,\n",
              " 'Air_Jordan_6': 9,\n",
              " 'Nike_Air_Force_1_Low': 10,\n",
              " 'Nike_Air_Huarache': 11,\n",
              " 'Nike_Air_Max_1': 12,\n",
              " 'Nike_Air_Max_90': 13,\n",
              " 'Nike_Air_Max_95': 14,\n",
              " 'Nike_Air_VaporMax': 15,\n",
              " 'Nike_Blazer': 16,\n",
              " 'Nike_Dunk_low': 17,\n",
              " 'Reebok_Instapump_Fury': 18,\n",
              " 'Unknown': 19}"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data_gen.class_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIWnqBOk5ABf",
        "outputId": "7caa7061-aec1-4c8f-bcd3-24c2b8c4a81e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 16, 12, 0]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('Adidas_Ultra_Boost_4.0', 'Nike_Blazer', 'Nike_Air_Max_1', 'Adidas_NMD_R1')"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_labels(preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkpIDKF58zFu",
        "outputId": "a36c5071-2493-4af4-84a8-52a6d1a3358a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  layer_config = serialize_layer_fn(layer)\n"
          ]
        }
      ],
      "source": [
        "model.save('sneaker_class_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jG48hCF8Bd5z"
      },
      "outputs": [],
      "source": [
        "model.save_weights('/content/drive/MyDrive/ebay_sneakers/please')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Sneakers YOLOv4-Darknet.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
